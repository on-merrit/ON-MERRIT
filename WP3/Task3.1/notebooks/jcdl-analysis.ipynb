{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will create plots for institutions of type universities only and for the period of 2007-2017. The input dataset contains info on universities as well as other institutions and for any period of time. The universities list comes from Times Higher Education (THE WUR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unpaywall dump used was from (April or June) 2018; hence analysis until 2017 only is going to be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unpaywall dump used was from (April or June) 2018; hence analysis until 2017 only is going to be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : What % of papers published by our selected universities in selected countries are Open Access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard path wrangling to be able to import project config and sources\n",
    "import os\n",
    "import sys\n",
    "from os.path import join\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print('Project root: {}'.format(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(join(root,\"spark/shared/\"))\n",
    "from MAG_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "\n",
    "# Installed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['pdf.fonttype'] = 42  # https://tex.stackexchange.com/a/508961/3741\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc,rcParams\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = None\n",
    "with open(join(root,\"spark/config.json\")) as fp:\n",
    "    cfg = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = join(root,\"documents/analysis/jcdl_dataset_question\")\n",
    "# Create a new directory to save results\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames_for_plot = {\n",
    "    \"austria\" : \"Austria\",\n",
    "    \"brazil\" : \"Brazil\",\n",
    "    \"germany\" : \"Germany\",\n",
    "    \"india\" : \"India\",\n",
    "    \"portugal\" : \"Portugal\",\n",
    "    \"russia\" : \"Russia\",\n",
    "    \"uk\" : \"UK\",\n",
    "    \"usa\" : \"USA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_OA_percent_bar_chart(oa_percent_dict, save_fname, x_label=None, plt_text=None, display_values=False, sort_by_keys=True, figuresize=(15,10), ylimit=[0,100]):\n",
    "    #     https://stackoverflow.com/a/37266356/530399\n",
    "    if sort_by_keys:\n",
    "        sorted_dict = sorted(oa_percent_dict.items(), key=lambda kv: kv[0]) # sorted by keys, return a list of tuples\n",
    "    else:\n",
    "        sorted_dict = sorted(oa_percent_dict.items(), key=lambda kv: kv[1]) # sorted by values\n",
    "    x, y = zip(*sorted_dict) # unpack a list of pairs into two tuples\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=figuresize)\n",
    "    \n",
    "    plt.bar(x,y)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label, fontsize=20)\n",
    "    ax.set_ylabel(\"Percentage of OA papers published\", fontsize=20)\n",
    "    \n",
    "    ax.xaxis.set_tick_params(labelsize=20)\n",
    "    ax.yaxis.set_tick_params(labelsize=20)\n",
    "    \n",
    "    ax.set_ylim(ylimit)\n",
    "    \n",
    "    if plt_text:\n",
    "#     https://stackoverflow.com/a/8482667/530399\n",
    "        plt.text(0.7, 0.9,plt_text, ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    if display_values:\n",
    "        for i, v in enumerate(y):\n",
    "            ax.text(i-.15, v + 2, str(round(v,2)), rotation=90, color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(x, rotation='vertical')\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=600)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=600)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_years = [2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A : Granularity Level of University Per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plt_univ_papers_OA_stats(country_papers_OA_df, univs_name):\n",
    "    univs_oa_percent = {} # needed for plot data\n",
    "    univs_info = {}\n",
    "    \n",
    "    univs_not_found = []\n",
    "    univs_found = []\n",
    "    \n",
    "    for org_univ_name in set(univs_name):  # remove duplicate univ names in the THE list, if any\n",
    "#         print(org_univ_name)\n",
    "\n",
    "        THE_univ_name_normalised = mag_normalisation_institution_names(org_univ_name)\n",
    "    \n",
    "        '''\n",
    "        The dataframe that will be selected for the current univ is either :\n",
    "        1. When the MAG normalizedname column matches to THE_univ_name_normalised\n",
    "        or\n",
    "        2. When the MAG normalised(wikiname) matches to THE_univ_name_normalised -- this matches English names (in MAG wiki links as well as THE) of non English name (in MAG normalisedname or displayname) universities.\n",
    "        '''\n",
    "        univ_papers_df_set1 = country_papers_OA_df[country_papers_OA_df['normalizedname']==THE_univ_name_normalised]\n",
    "        \n",
    "        univ_papers_df_set2 = country_papers_OA_df[country_papers_OA_df['normalizedwikiname']==THE_univ_name_normalised]\n",
    "        \n",
    "        # The records in two sets can be the excatly the same \n",
    "        # Concat and remove exact duplicates  -- https://stackoverflow.com/a/21317570/530399\n",
    "        univ_papers_df = pd.concat([univ_papers_df_set1, univ_papers_df_set2]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "\n",
    "#         Put additional criteria that these papers are from 2007 till 2017\n",
    "        univ_papers_df = univ_papers_df[univ_papers_df['year'].isin(study_years)]\n",
    "        \n",
    "        \n",
    "        # Same paper will have multiple entries if there are multiple authors for that paper from same university.\n",
    "        # This is not necessary because the input dataset was already prepared to exclude such duplicates.\n",
    "#         univ_papers_df = univ_papers_df.drop_duplicates(subset=\"paperid\")\n",
    "\n",
    "\n",
    "        \n",
    "        count_total_univ_papers = len(univ_papers_df)\n",
    "        \n",
    "        \n",
    "        # For those I couldn't match/find their name, it is not fair to say that their OA count is 0. Should be excluded from the graph.\n",
    "        if count_total_univ_papers==0:\n",
    "            univs_not_found.append(org_univ_name+\"    @    \"+THE_univ_name_normalised)\n",
    "        else:\n",
    "            univs_found.append(org_univ_name)\n",
    "            \n",
    "            univs_info[org_univ_name] = {}\n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "            # All (OA + unknown) \n",
    "            count_all_2007 = len(univ_papers_df[univ_papers_df['year']==2007])\n",
    "            count_all_2008 = len(univ_papers_df[univ_papers_df['year']==2008])\n",
    "            count_all_2009 = len(univ_papers_df[univ_papers_df['year']==2009])\n",
    "            count_all_2010 = len(univ_papers_df[univ_papers_df['year']==2010])\n",
    "            count_all_2011 = len(univ_papers_df[univ_papers_df['year']==2011])\n",
    "            count_all_2012 = len(univ_papers_df[univ_papers_df['year']==2012])\n",
    "            count_all_2013 = len(univ_papers_df[univ_papers_df['year']==2013])\n",
    "            count_all_2014 = len(univ_papers_df[univ_papers_df['year']==2014])\n",
    "            count_all_2015 = len(univ_papers_df[univ_papers_df['year']==2015])\n",
    "            count_all_2016 = len(univ_papers_df[univ_papers_df['year']==2016])\n",
    "            count_all_2017 = len(univ_papers_df[univ_papers_df['year']==2017])\n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"yearwise_all\"] = {}\n",
    "            univs_info[org_univ_name][\"yearwise_all\"][\"count_year\"] = {\n",
    "                \"2007\":count_all_2007,\n",
    "                \"2008\":count_all_2008, \"2009\":count_all_2009, \"2010\":count_all_2010,\"2011\":count_all_2011,\n",
    "                \"2012\":count_all_2012,\"2013\":count_all_2013,\n",
    "                \"2014\":count_all_2014,\"2015\":count_all_2015,\n",
    "                \"2016\":count_all_2016,\"2017\":count_all_2017\n",
    "            }\n",
    "            \n",
    "            \n",
    "            \n",
    "            # OA part\n",
    "            OA_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']==\"true\"] # stored as a string in csv\n",
    "            unknown_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']!=\"true\"] # stored as a string in csv\n",
    "            \n",
    "            count_OA_univ_papers = len(OA_univ_papers_df)\n",
    "            count_unknown_univ_papers = len(unknown_univ_papers_df)\n",
    "\n",
    "            univ_oa_percent = (count_OA_univ_papers*100.00)/count_total_univ_papers\n",
    "            univ_other_percent = (count_unknown_univ_papers*100.00)/count_total_univ_papers\n",
    "            \n",
    "            univs_oa_percent[org_univ_name] = univ_oa_percent\n",
    "        \n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"count_OA_papers\"] = count_OA_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_OA_papers\"] = univ_oa_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_unknown_papers\"] = count_unknown_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_unknown_papers\"] = univ_other_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Further to get a yearwise breakdown of oa papers\n",
    "            univs_info[org_univ_name][\"yearwise_OA\"] = {}            \n",
    "            \n",
    "            count_oa_2007 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2007])\n",
    "            count_oa_2008 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2008])\n",
    "            count_oa_2009 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2009])\n",
    "            count_oa_2010 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2010])\n",
    "            count_oa_2011 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2011])\n",
    "            count_oa_2012 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2012])\n",
    "            count_oa_2013 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2013])\n",
    "            count_oa_2014 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2014])\n",
    "            count_oa_2015 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2015])\n",
    "            count_oa_2016 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2016])\n",
    "            count_oa_2017 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2017])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"yearwise_OA\"][\"count_year\"] = {\"2007\":count_oa_2007, \"2008\":count_oa_2008,\n",
    "                                                                  \"2009\":count_oa_2009, \"2010\":count_oa_2010,\n",
    "                                                                  \"2011\":count_oa_2011, \"2012\":count_oa_2012,\n",
    "                                                                  \"2013\":count_oa_2013, \"2014\":count_oa_2014,\n",
    "                                                                  \"2015\":count_oa_2015, \"2016\":count_oa_2016,\n",
    "                                                                  \"2017\":count_oa_2017}\n",
    "    \n",
    "    bar_fig = create_OA_percent_bar_chart(univs_oa_percent, save_fname = join(output_dir,country_name+\"_\"+'OA_percent') , x_label = (\"Universities in \"+country_name), plt_text = ('Total Count of Universities = '+str(len(univs_oa_percent))) )\n",
    "    return bar_fig, univs_info, univs_not_found, univs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries_plot = {}\n",
    "all_countries_all_univs_OA_info = {}\n",
    "all_countries_univs_found_not_found = {}\n",
    "\n",
    "for country_name,univs_name in cfg['data']['all_THE_WUR_institutions_by_country'].items():\n",
    "    print(\"\\nProcesing for dataset of univs in \"+country_name+\"\\n\")\n",
    "    all_countries_plot[country_name] = {}\n",
    "    all_countries_univs_found_not_found[country_name] =  {}\n",
    "    \n",
    "    # CSV has repeated header from multiple partitions of the merge on pyspark csv output. Hence need to treat as string.\n",
    "    country_papers_OA_df = pd.read_csv(join(root,\"data/processed/OA_status_\"+country_name+\"_papers.csv\"), header=0, sep=\",\", dtype={'is_OA': object, \"url_lists_as_string\": object, \"year\": object, \"wikipage\": object})  # object means string\n",
    "    # Then eliminate problematic lines\n",
    "    #  temp fix until spark csv merge header issue is resolved -- the header line is present in each re-partition's output csv\n",
    "    country_papers_OA_df.drop(country_papers_OA_df[country_papers_OA_df.paperid == \"paperid\"].index, inplace=True)\n",
    "    # Then reset dtypes as needed.\n",
    "    country_papers_OA_df = country_papers_OA_df.astype({'year':int})  # todo : for other types too including is_OA and update the check method to boolean type\n",
    "    \n",
    "    # Finally, create a new column named normalizedwikiname. This is helpful for matching english names of non-english universities. Eg: get \"federal university of health sciences of porto alegre\" for \"universidade federal de ciencias da saude de porto alegre\" using the wikilink which contains \"universidade federal de ciencias da saude de porto alegre\" in it.\n",
    "    country_papers_OA_df[\"normalizedwikiname\"] = country_papers_OA_df['wikipage'].apply(mag_normalisation_wiki_link)\n",
    "    \n",
    "    \n",
    "    country_plot, univs_info, univs_not_found, univs_found = get_plt_univ_papers_OA_stats(country_papers_OA_df, univs_name)\n",
    "    \n",
    "    all_countries_plot[country_name] =  country_plot\n",
    "    all_countries_all_univs_OA_info[country_name] =  univs_info\n",
    "    \n",
    "    count_total_univs = len(univs_not_found) + len(univs_found)\n",
    "    \n",
    "    not_found_details = {}\n",
    "    not_found_details['univ_names'] = univs_not_found\n",
    "    not_found_details['count_univs'] = len(univs_not_found)\n",
    "    not_found_details['percent_univs'] = (len(univs_not_found)*100.00)/count_total_univs\n",
    "    \n",
    "    found_details = {}\n",
    "    found_details['univ_names'] = univs_found\n",
    "    found_details['count_univs'] = len(univs_found)\n",
    "    found_details['percent_univs'] = (len(univs_found)*100.00)/count_total_univs\n",
    "    \n",
    "    \n",
    "    all_details = {}\n",
    "    all_details['count_univs'] = count_total_univs\n",
    "    \n",
    "    all_countries_univs_found_not_found[country_name]['not_found'] = not_found_details\n",
    "    all_countries_univs_found_not_found[country_name]['found'] = found_details\n",
    "    all_countries_univs_found_not_found[country_name]['all'] = all_details\n",
    "    \n",
    "    \n",
    "        \n",
    "    print(\"Saved plot for dataset of \"+country_name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text files with the infos\n",
    "\n",
    "with open(join(output_dir,'all_countries_univs_found_not_found.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_univs_found_not_found, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "        \n",
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_all_univs_OA_info, sort_keys=True, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from previously saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt')) as file:\n",
    "     all_countries_all_univs_OA_info = json.load(file)\n",
    "        \n",
    "# all_countries_all_univs_OA_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Representative universities OA percent comparision Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representative_univs_line_plot_groups(all_countries_all_univs_OA_info, save_fname, x_label=None, y_label = \"Percentage of OA Papers Published\", plt_text=None):\n",
    "\n",
    "    country_rep_univs = {}\n",
    "    \n",
    "    width = 0.9\n",
    "    \n",
    "    colors = (\"red\", \"blue\", \"green\")\n",
    "    groups = (\"Low Research Intensive Universities\", \"Medium Research Intensive Universities\", \"High Research Intensive Universities\")\n",
    "    \n",
    "    \n",
    "    high_tier_plot_data = []\n",
    "    mid_tier_plot_data = []\n",
    "    low_tier_plot_data = []\n",
    "    \n",
    "    \n",
    "    country_tier_mean_values = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for country, univ_tiers in cfg[\"data\"][\"research_intensive_THE_WUR_institutions_by_country\"].items():\n",
    "#         print(country)\n",
    "        country_rep_univs[cnames_for_plot[country]] = {}\n",
    "    \n",
    "        country_rep_univs[cnames_for_plot[country]][\"High_Tier\"]={}\n",
    "        country_rep_univs[cnames_for_plot[country]][\"Mid_Tier\"]={}\n",
    "        country_rep_univs[cnames_for_plot[country]][\"Low_Tier\"]={}\n",
    "        \n",
    "\n",
    "        high_tier_univs = univ_tiers[\"high\"]\n",
    "        for x in high_tier_univs:\n",
    "            high_tier_plot_data.append((country+\"(High)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"High_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "            \n",
    "    \n",
    "        medium_tier_univs = univ_tiers[\"medium\"]\n",
    "        for x in medium_tier_univs:\n",
    "            mid_tier_plot_data.append((country+\"(Mid)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"Mid_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "            \n",
    "        \n",
    "        low_tier_univs = univ_tiers[\"low\"]\n",
    "        for x in low_tier_univs:\n",
    "            low_tier_plot_data.append((country+\"(Low)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"Low_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(15,10), sharex=True, sharey=True)\n",
    "    \n",
    "    sorted_cnames = sorted(cfg[\"data\"][\"research_intensive_THE_WUR_institutions_by_country\"].keys())\n",
    "    \n",
    "    hidden_tick_indices = []\n",
    "    count_hidden_tick_index = -1\n",
    "    for i in range(len(sorted_cnames)):\n",
    "        cname = sorted_cnames[i]\n",
    "        \n",
    "        # First plot the data for low tier univs of the country\n",
    "        country_low_tier_univs_values = [x[1] for x in low_tier_plot_data if x[0]==cname+\"(Low)\"]\n",
    "#         axs.plot([cname+\"(Low)\"]*len(country_low_tier_univs_values), country_low_tier_univs_values, c=\"red\", label=\"Low Tier University\", linestyle='-', marker='o', linewidth=4)\n",
    "\n",
    "        \n",
    "        country_low_tier_mean_value = mean(country_low_tier_univs_values)\n",
    "        country_low_tier_min_value = min(country_low_tier_univs_values)\n",
    "        country_low_tier_max_value = max(country_low_tier_univs_values)\n",
    "        country_tier_mean_values.append((cname+\"(Low)\",country_low_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"Low_Tier\"][\"Mean\"] = country_low_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cname+\"(Low)\"]*len(country_low_tier_univs_values), country_low_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_low_tier_max_value - country_low_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+1-width/2,country_low_tier_min_value-1) ,width=width, height=height+2, linewidth=1, color='cornflowerblue', fill=\"cornflowerblue\", alpha=0.25, label=\"Low Tier Universities\"))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Then plot the data for mid tier univs of the country    \n",
    "        country_mid_tier_univs_values = [x[1] for x in mid_tier_plot_data if x[0]==cname+\"(Mid)\"]\n",
    "#         axs.plot([cnames_for_plot[cname]]*len(country_mid_tier_univs_values), country_mid_tier_univs_values, c=\"orange\", label=\"Mid Tier University\", linestyle='-', marker='o', linewidth=4)  # to make this tick mark visible as cname rather than the true cname_mid; also capitalize the first letter\n",
    "\n",
    "        \n",
    "    \n",
    "        country_mid_tier_mean_value = mean(country_mid_tier_univs_values)\n",
    "        country_mid_tier_min_value = min(country_mid_tier_univs_values)\n",
    "        country_mid_tier_max_value = max(country_mid_tier_univs_values)\n",
    "        country_tier_mean_values.append((cnames_for_plot[cname],country_mid_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"Mid_Tier\"][\"Mean\"] = country_mid_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cnames_for_plot[cname]]*len(country_mid_tier_univs_values), country_mid_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_mid_tier_max_value - country_mid_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+2-width/2,country_mid_tier_min_value-1) ,width=width, height=height+2, linewidth=1, color='orange', fill=\"orange\", alpha=0.25, label=\"Mid Tier Universities\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # Also, plot the data for high tier univs of the country\n",
    "        country_high_tier_univs_values = [x[1] for x in high_tier_plot_data if x[0]==cname+\"(High)\"]\n",
    "#         axs.plot([cname+\"(High)\"]*len(country_high_tier_univs_values), country_high_tier_univs_values, c=\"green\", label=\"High Tier University\", linestyle='-', marker='o', linewidth=4)\n",
    "        \n",
    "        \n",
    "        country_high_tier_mean_value = mean(country_high_tier_univs_values)\n",
    "        country_high_tier_min_value = min(country_high_tier_univs_values)\n",
    "        country_high_tier_max_value = max(country_high_tier_univs_values)\n",
    "        country_tier_mean_values.append((cname+\"(High)\",country_high_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"High_Tier\"][\"Mean\"] = country_high_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cname+\"(High)\"]*len(country_high_tier_univs_values), country_high_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_high_tier_max_value - country_high_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+3-width/2,country_high_tier_min_value-1),width=width, height=height+2, linewidth=1, color='green', fill=\"green\", alpha=0.25, label=\"High Tier Universities\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Hide the tick marks for the low and high tier markers\n",
    "        hidden_tick_indices.append(count_hidden_tick_index+1)  # low marker\n",
    "        hidden_tick_indices.append(count_hidden_tick_index+3)  # high marker\n",
    "        \n",
    "        \n",
    "        # Finally add three fake tick points for inter spacing among the groups\n",
    "        if i!=(len(sorted_cnames)-1):  # except when the last true xticks have been added.\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 4\n",
    "            axs.plot([cname+\"(None1)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 1\n",
    "            axs.plot([cname+\"(None2)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 1\n",
    "            axs.plot([cname+\"(None3)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "        \n",
    "        \n",
    "    \n",
    "#     https://stackoverflow.com/a/13583251/530399\n",
    "    xticks = axs.xaxis.get_major_ticks()\n",
    "    for hidden_tick_index in hidden_tick_indices:\n",
    "        xticks[hidden_tick_index].set_visible(False)\n",
    "    \n",
    "    \n",
    "    # Plot the mean value line\n",
    "#     axs.scatter(*zip(*country_tier_mean_values), label='Mean Value', s=280, facecolors='none', edgecolors='b')\n",
    "    axs.scatter(*zip(*country_tier_mean_values), label='Mean OA %', c=\"red\", marker='s', s=104)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # show grid at every ticks\n",
    "#     plt.grid()\n",
    "# https://stackoverflow.com/a/39039520/530399\n",
    "    axs.set_axisbelow(True)\n",
    "    axs.yaxis.grid(color='lightgrey', linestyle='dashed')\n",
    "    \n",
    "\n",
    "    # Frequency of y-ticks\n",
    "    # https://stackoverflow.com/a/12608937/530399\n",
    "    stepsize=3\n",
    "    start, end = axs.get_ylim()\n",
    "    axs.yaxis.set_ticks(np.arange(1, end, stepsize))\n",
    "    \n",
    "    # Font size to use for ticks\n",
    "    axs.xaxis.set_tick_params(labelsize=20)\n",
    "    axs.yaxis.set_tick_params(labelsize=20)\n",
    "    \n",
    "\n",
    "    axs.set_ylabel(y_label, fontsize=24, labelpad=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #     Remove multiple legends by unique entires. Because each country was separately adeed for each tiers, there are duplicate legend entries.\n",
    "#     https://stackoverflow.com/a/13589144/530399\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), prop={'size': 16},\n",
    "               loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True\n",
    "              )  # location of legend -- https://stackoverflow.com/a/4701285/530399\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return fig, country_rep_univs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_univ_OA_plot, country_rep_univs_data = create_representative_univs_line_plot_groups(all_countries_all_univs_OA_info, save_fname = join(output_dir,\"all_countries_representative_univs_OA_percent\"))\n",
    "\n",
    "rep_univ_OA_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write country_rep_univs to file\n",
    "with open(join(output_dir,'representative_univs_in_all_countries.txt'), 'w') as file:\n",
    "     file.write(json.dumps(country_rep_univs_data, sort_keys=True, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Analysis at Country Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can't build up on the data from univ_level because of duplicate paper. If the same paper(paperid) has authors from multiple univs within the same country, only one instance of it can be considered. \n",
    "\n",
    "#### 1. Load country level dataset 2. Retain records from unis in THE_WUR list only. 3. Delete duplicate paperid records 4. records from study_years only 4. Yearwise Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_info = {}\n",
    "countries_oa_percents = {}  # needed for plot.\n",
    "\n",
    "for country_name,univs_name in cfg['data']['all_THE_WUR_institutions_by_country'].items():\n",
    "    \n",
    "    countries_oa_info[country_name] = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. Load Data\n",
    "    # CSV has repeated header from multiple partitions of the merge on pyspark csv output. Hence need to treat as string.\n",
    "    country_papers_OA_df = pd.read_csv(join(root,\"data/processed/OA_status_\"+country_name+\"_papers.csv\"), header=0, sep=\",\", dtype={'is_OA': object, \"url_lists_as_string\": object, \"year\": object, \"wikipage\": object})  # object means string\n",
    "    # Then eliminate problematic lines\n",
    "    #  temp fix until spark csv merge header issue is resolved -- the header line is present in each re-partition's output csv\n",
    "    country_papers_OA_df.drop(country_papers_OA_df[country_papers_OA_df.paperid == \"paperid\"].index, inplace=True)\n",
    "    # Then reset dtypes as needed.\n",
    "    country_papers_OA_df = country_papers_OA_df.astype({'year':int})  # todo : for other types too including is_OA and update the check method to boolean type\n",
    "    \n",
    "    \n",
    "    # Finally, create a new column named normalizedwikiname. This is helpful for matching english names of non-english universities. Eg: get \"federal university of health sciences of porto alegre\" for \"universidade federal de ciencias da saude de porto alegre\" using the wikilink which contains \"universidade federal de ciencias da saude de porto alegre\" in it.\n",
    "    country_papers_OA_df[\"normalizedwikiname\"] = country_papers_OA_df['wikipage'].apply(mag_normalisation_wiki_link)\n",
    "    \n",
    "    \n",
    "    # 2. Retain records from THE_WUR only\n",
    "    univs_names_normalized = [mag_normalisation_institution_names(x) for x in univs_name]\n",
    "    country_THE_papers_OA_df_set1 = country_papers_OA_df[country_papers_OA_df['normalizedname'].isin(univs_names_normalized)]\n",
    "    country_THE_papers_OA_df_set2 = country_papers_OA_df[country_papers_OA_df['normalizedwikiname'].isin(univs_names_normalized)]\n",
    "    # The records in two sets can be the excatly the same \n",
    "# Concat and remove exact duplicates  -- https://stackoverflow.com/a/21317570/530399\n",
    "    country_THE_papers_OA_df = pd.concat([country_THE_papers_OA_df_set1, country_THE_papers_OA_df_set2]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # 3. Remove Duplicates paperids -- same paper with authors from multiple universities within the country.\n",
    "    country_THE_papers_OA_df = country_THE_papers_OA_df.drop_duplicates(subset=\"paperid\")\n",
    "    \n",
    "    # 4. Put criteria that these papers are from 2007 till 2017\n",
    "    country_THE_papers_OA_df = country_THE_papers_OA_df[country_THE_papers_OA_df['year'].isin(study_years)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    OA_papers = country_THE_papers_OA_df[country_THE_papers_OA_df['is_OA']==\"true\"]\n",
    "    unknown_papers = country_THE_papers_OA_df[country_THE_papers_OA_df['is_OA']!=\"true\"]\n",
    "    \n",
    "    \n",
    "    count_country_OA_papers = len(OA_papers)\n",
    "    count_country_unknown_papers = len(unknown_papers)\n",
    "    \n",
    "    total_country_papers = count_country_OA_papers + count_country_unknown_papers\n",
    "    percent_OA_country = (count_country_OA_papers * 100.00)/total_country_papers\n",
    "    percent_unknown_country = (count_country_unknown_papers * 100.00)/total_country_papers\n",
    "    \n",
    "    \n",
    "    countries_oa_percents[country_name] = percent_OA_country\n",
    "    \n",
    "    countries_oa_info[country_name]['count_OA_papers'] = count_country_OA_papers\n",
    "    countries_oa_info[country_name]['count_unknown_papers'] = count_country_unknown_papers    \n",
    "    countries_oa_info[country_name]['percent_OA_papers'] = percent_OA_country\n",
    "    countries_oa_info[country_name]['percent_unknown_papers'] = percent_unknown_country\n",
    "    countries_oa_info[country_name]['count_total_papers'] = total_country_papers\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Yearwise Breakdown\n",
    "    count_oa_2007 = len(OA_papers[OA_papers[\"year\"]==2007])\n",
    "    count_oa_2008 = len(OA_papers[OA_papers[\"year\"]==2008])\n",
    "    count_oa_2009 = len(OA_papers[OA_papers[\"year\"]==2009])\n",
    "    count_oa_2010 = len(OA_papers[OA_papers[\"year\"]==2010])\n",
    "    count_oa_2011 = len(OA_papers[OA_papers[\"year\"]==2011])\n",
    "    count_oa_2012 = len(OA_papers[OA_papers[\"year\"]==2012])\n",
    "    count_oa_2013 = len(OA_papers[OA_papers[\"year\"]==2013])\n",
    "    count_oa_2014 = len(OA_papers[OA_papers[\"year\"]==2014])\n",
    "    count_oa_2015 = len(OA_papers[OA_papers[\"year\"]==2015])\n",
    "    count_oa_2016 = len(OA_papers[OA_papers[\"year\"]==2016])\n",
    "    count_oa_2017 = len(OA_papers[OA_papers[\"year\"]==2017])\n",
    "   \n",
    "\n",
    "    \n",
    "    count_all_2007 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2007])\n",
    "    count_all_2008 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2008])\n",
    "    count_all_2009 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2009])\n",
    "    count_all_2010 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2010])\n",
    "    count_all_2011 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2011])\n",
    "    count_all_2012 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2012])\n",
    "    count_all_2013 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2013])\n",
    "    count_all_2014 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2014])\n",
    "    count_all_2015 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2015])\n",
    "    count_all_2016 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2016])\n",
    "    count_all_2017 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2017])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    countries_oa_info[country_name][\"yearwise_OA\"] = {}\n",
    "    \n",
    "    countries_oa_info[country_name][\"yearwise_OA\"][\"count_year\"] = {\"2007\":count_oa_2007, \"2008\":count_oa_2008,\n",
    "                                                           \"2009\":count_oa_2009, \"2010\":count_oa_2010,\n",
    "                                                           \"2011\":count_oa_2011, \"2012\":count_oa_2012,\n",
    "                                                           \"2013\":count_oa_2013, \"2014\":count_oa_2014,\n",
    "                                                           \"2015\":count_oa_2015, \"2016\":count_oa_2016,\n",
    "                                                           \"2017\":count_oa_2017}\n",
    "\n",
    "    \n",
    "    # Lets find the percentage OA in each year\n",
    "    countries_oa_info[country_name][\"yearwise_OA\"][\"percent_year\"] = {\n",
    "        \"2007\":(count_oa_2007*100.00)/count_all_2007,\n",
    "        \"2008\":(count_oa_2008*100.00)/count_all_2008,\n",
    "       \"2009\":(count_oa_2009*100.00)/count_all_2009,\n",
    "        \"2010\":(count_oa_2010*100.00)/count_all_2010,\n",
    "       \"2011\":(count_oa_2011*100.00)/count_all_2011,\n",
    "        \"2012\":(count_oa_2012*100.00)/count_all_2012,\n",
    "       \"2013\":(count_oa_2013*100.00)/count_all_2013,\n",
    "        \"2014\":(count_oa_2014*100.00)/count_all_2014,\n",
    "       \"2015\":(count_oa_2015*100.00)/count_all_2015, \n",
    "        \"2016\":(count_oa_2016*100.00)/count_all_2016,\n",
    "       \"2017\":(count_oa_2017*100.00)/count_all_2017\n",
    "    }\n",
    "    \n",
    "    \n",
    "    print(\"\\nCompleted processing for dataset of \"+country_name+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(output_dir,'all_countries_OA_info.txt'), 'w') as file:\n",
    "     file.write(json.dumps(countries_oa_info, sort_keys=True, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percent_bar_plot = create_OA_percent_bar_chart({cnames_for_plot[key]:value for key, value in countries_oa_percents.items()}, save_fname = join(output_dir,\"all_countries_OA_percent\"), x_label = \"Countries\", display_values=True, sort_by_keys=False, figuresize=(8,8), ylimit=[0,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percent_bar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yearwise_OA_percent_line_chart(countries_oa_info, save_fname, x_label = \"Year\", plt_text=None):\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    country_names_list = []\n",
    "    markers = ['o', 'x', 'v', 's', '*', '+', 'D', '|']\n",
    "    for country_name,oa_info in countries_oa_info.items():\n",
    "        \n",
    "        percent_oa = oa_info[\"yearwise_OA\"][\"percent_year\"]\n",
    "        # sort by year\n",
    "        #  https://stackoverflow.com/a/37266356/530399\n",
    "        sort_by_year = sorted(percent_oa.items(), key=lambda kv: int(kv[0]))\n",
    "        years, percent_oas = zip(*sort_by_year) # unpack a list of pairs into two tuples\n",
    "        \n",
    "        plt.plot(years, percent_oas, linewidth=4, markersize=12, marker=markers[len(country_names_list)])\n",
    "        \n",
    "        country_names_list.append(country_name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label, fontsize=20, labelpad=10)\n",
    "    ax.set_ylabel(\"% of OA paper published in each year\", fontsize=24, labelpad=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Font size to use for ticks\n",
    "    ax.xaxis.set_tick_params(labelsize=20)\n",
    "    ax.yaxis.set_tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    # Frequency of x-ticks\n",
    "    # https://stackoverflow.com/a/12608937/530399\n",
    "    stepsize=3\n",
    "    start, end = ax.get_ylim()\n",
    "    ax.yaxis.set_ticks(np.arange(int(start), end, stepsize))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # show grid at every ticks\n",
    "    #     plt.grid()\n",
    "    # https://stackoverflow.com/a/39039520/530399\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='lightgrey', linestyle='dashed')\n",
    "    \n",
    "    \n",
    "    if plt_text:\n",
    "#     https://stackoverflow.com/a/8482667/530399\n",
    "        plt.text(0.7, 0.9,plt_text, ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "#     plt.xticks(years)\n",
    "    plt.legend([cnames_for_plot[x] for x in country_names_list], loc='upper left', prop={'size': 16})\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_OA_percent_each_year_line_plot = create_yearwise_OA_percent_line_chart(countries_oa_info, save_fname = join(output_dir,\"all_countries_OA_percent_each_year\"), x_label = \"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_OA_percent_each_year_line_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_info['usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_oa_info['brazil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_oa_info['germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\nCompleted!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
