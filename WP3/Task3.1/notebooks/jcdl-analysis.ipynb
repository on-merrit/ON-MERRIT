{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will create plots for institutions of type universities only. The universities list comes from Times Higher Education (THE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unpaywall dump used was from (April or June) 2018; hence analysis until 2017 only is going to be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : What % of papers published by our selected universities in selected countries are Open Access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard path wrangling to be able to import project config and sources\n",
    "import os\n",
    "import sys\n",
    "from os.path import join\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print('Project root: {}'.format(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(join(root,\"spark/shared/\"))\n",
    "from MAG_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "\n",
    "# Installed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc,rcParams\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from statistics import mean \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "So as to enable font properties on matplotlib plot ticks and labels\n",
    "https://stackoverflow.com/a/29772534/530399\n",
    "'''\n",
    "# # activate latex text rendering\n",
    "# rc('text', usetex=True)\n",
    "# rc('axes', linewidth=2)\n",
    "# rc('font', weight='bold')\n",
    "# rcParams['text.latex.preamble'] = [r'\\usepackage{sfmath} \\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = None\n",
    "with open(join(root,\"spark/config.json\")) as fp:\n",
    "    cfg = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = join(root,\"documents/analysis/jcdl_dataset_question\")\n",
    "# Create a new directory to save results\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames_for_plot = {\n",
    "    \"austria\" : \"Austria\",\n",
    "    \"brazil\" : \"Brazil\",\n",
    "    \"germany\" : \"Germany\",\n",
    "    \"india\" : \"India\",\n",
    "    \"portugal\" : \"Portugal\",\n",
    "    \"russia\" : \"Russia\",\n",
    "    \"uk\" : \"UK\",\n",
    "    \"usa\" : \"USA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_OA_percent_bar_chart(oa_percent_dict, save_fname, x_label=None, plt_text=None, display_values=False):\n",
    "    #     https://stackoverflow.com/a/37266356/530399\n",
    "    sort_by_vals = sorted(oa_percent_dict.items(), key=lambda kv: kv[0]) # sorted by keys, return a list of tuples\n",
    "    x, y = zip(*sort_by_vals) # unpack a list of pairs into two tuples\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    plt.bar(x,y)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Percentage of OA papers published\")\n",
    "    \n",
    "    ax.set_ylim([0,100])\n",
    "    \n",
    "    if plt_text:\n",
    "#     https://stackoverflow.com/a/8482667/530399\n",
    "        plt.text(0.7, 0.9,plt_text, ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    if display_values:\n",
    "        for i, v in enumerate(y):\n",
    "            ax.text(i-.25, v + 3, str(round(v,3)), color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(x, rotation='vertical')\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=600)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=600)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_wiki_link_normalise(wikilink):\n",
    "#     Get the english name from the wiki\n",
    "\n",
    "#     print(wikilink)\n",
    "    try:\n",
    "        last_slash_index = wikilink.rindex('/')\n",
    "        start_index = last_slash_index+1\n",
    "        uni_name = wikilink[start_index:]\n",
    "    except:\n",
    "        uni_name = \"\"\n",
    "#     Apply MAG normalisation to the name\n",
    "    return mag_normalisation_institution_names(uni_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_years = [2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A : Granularity Level of University Per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plt_univ_papers_OA_stats(country_papers_OA_df, univs_name):\n",
    "    univs_oa_percent = {} # needed for plot data\n",
    "    univs_info = {}\n",
    "    \n",
    "    univs_not_found = []\n",
    "    univs_found = []\n",
    "    \n",
    "    for org_univ_name in set(univs_name):  # remove duplicate univ names in the THE list, if any\n",
    "#         print(org_univ_name)\n",
    "\n",
    "        THE_univ_name_normalised = mag_normalisation_institution_names(org_univ_name)\n",
    "    \n",
    "        '''\n",
    "        The dataframe that will be selected for the current univ is either :\n",
    "        1. When the MAG normalizedname column matches to THE_univ_name_normalised\n",
    "        or\n",
    "        2. When the MAG normalised(wikiname) matches to THE_univ_name_normalised -- this matches English names (in MAG wiki links as well as THE) of non English name (in MAG normalisedname or displayname) universities.\n",
    "        '''\n",
    "        univ_papers_df_set1 = country_papers_OA_df[country_papers_OA_df['normalizedname']==THE_univ_name_normalised]\n",
    "        \n",
    "        univ_papers_df_set2 = country_papers_OA_df[country_papers_OA_df['normalizedwikiname']==THE_univ_name_normalised]\n",
    "        \n",
    "        # The records in two sets can be the excatly the same \n",
    "        # Concat and remove exact duplicates  -- https://stackoverflow.com/a/21317570/530399\n",
    "        univ_papers_df = pd.concat([univ_papers_df_set1, univ_papers_df_set2]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "\n",
    "#         Put additional criteria that these papers are from 2007 till 2017\n",
    "        univ_papers_df = univ_papers_df[univ_papers_df['year'].isin(study_years)]\n",
    "        \n",
    "        \n",
    "        # Same paper will have multiple entries if there are multiple authors for that paper from same university.\n",
    "        # This is not necessary because the input dataset was already prepared to exclude such duplicates.\n",
    "#         univ_papers_df = univ_papers_df.drop_duplicates(subset=\"paperid\")\n",
    "\n",
    "\n",
    "        \n",
    "        count_total_univ_papers = len(univ_papers_df)\n",
    "        \n",
    "        \n",
    "        # For those I couldn't match/find their name, it is not fair to say that their OA count is 0. Should be excluded from the graph.\n",
    "        if count_total_univ_papers==0:\n",
    "            univs_not_found.append(org_univ_name+\"    @    \"+THE_univ_name_normalised)\n",
    "        else:\n",
    "            univs_found.append(org_univ_name)\n",
    "            \n",
    "            univs_info[org_univ_name] = {}\n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "            # All (OA + unknown) \n",
    "            count_all_2007 = len(univ_papers_df[univ_papers_df['year']==2007])\n",
    "            count_all_2008 = len(univ_papers_df[univ_papers_df['year']==2008])\n",
    "            count_all_2009 = len(univ_papers_df[univ_papers_df['year']==2009])\n",
    "            count_all_2010 = len(univ_papers_df[univ_papers_df['year']==2010])\n",
    "            count_all_2011 = len(univ_papers_df[univ_papers_df['year']==2011])\n",
    "            count_all_2012 = len(univ_papers_df[univ_papers_df['year']==2012])\n",
    "            count_all_2013 = len(univ_papers_df[univ_papers_df['year']==2013])\n",
    "            count_all_2014 = len(univ_papers_df[univ_papers_df['year']==2014])\n",
    "            count_all_2015 = len(univ_papers_df[univ_papers_df['year']==2015])\n",
    "            count_all_2016 = len(univ_papers_df[univ_papers_df['year']==2016])\n",
    "            count_all_2017 = len(univ_papers_df[univ_papers_df['year']==2017])\n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"yearwise_all\"] = {}\n",
    "            univs_info[org_univ_name][\"yearwise_all\"][\"count_year\"] = {\n",
    "                \"2007\":count_all_2007,\n",
    "                \"2008\":count_all_2008, \"2009\":count_all_2009, \"2010\":count_all_2010,\"2011\":count_all_2011,\n",
    "                \"2012\":count_all_2012,\"2013\":count_all_2013,\n",
    "                \"2014\":count_all_2014,\"2015\":count_all_2015,\n",
    "                \"2016\":count_all_2016,\"2017\":count_all_2017\n",
    "            }\n",
    "            \n",
    "            \n",
    "            \n",
    "            # OA part\n",
    "            OA_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']==\"true\"] # stored as a string in csv\n",
    "            unknown_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']!=\"true\"] # stored as a string in csv\n",
    "            \n",
    "            count_OA_univ_papers = len(OA_univ_papers_df)\n",
    "            count_unknown_univ_papers = len(unknown_univ_papers_df)\n",
    "\n",
    "            univ_oa_percent = (count_OA_univ_papers*100.00)/count_total_univ_papers\n",
    "            univ_other_percent = (count_unknown_univ_papers*100.00)/count_total_univ_papers\n",
    "            \n",
    "            univs_oa_percent[org_univ_name] = univ_oa_percent\n",
    "        \n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"count_OA_papers\"] = count_OA_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_OA_papers\"] = univ_oa_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_unknown_papers\"] = count_unknown_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_unknown_papers\"] = univ_other_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Further to get a yearwise breakdown of oa papers\n",
    "            univs_info[org_univ_name][\"yearwise_OA\"] = {}            \n",
    "            \n",
    "            count_oa_2007 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2007])\n",
    "            count_oa_2008 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2008])\n",
    "            count_oa_2009 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2009])\n",
    "            count_oa_2010 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2010])\n",
    "            count_oa_2011 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2011])\n",
    "            count_oa_2012 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2012])\n",
    "            count_oa_2013 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2013])\n",
    "            count_oa_2014 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2014])\n",
    "            count_oa_2015 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2015])\n",
    "            count_oa_2016 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2016])\n",
    "            count_oa_2017 = len(OA_univ_papers_df[OA_univ_papers_df['year']==2017])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"yearwise_OA\"][\"count_year\"] = {\"2007\":count_oa_2007, \"2008\":count_oa_2008,\n",
    "                                                                  \"2009\":count_oa_2009, \"2010\":count_oa_2010,\n",
    "                                                                  \"2011\":count_oa_2011, \"2012\":count_oa_2012,\n",
    "                                                                  \"2013\":count_oa_2013, \"2014\":count_oa_2014,\n",
    "                                                                  \"2015\":count_oa_2015, \"2016\":count_oa_2016,\n",
    "                                                                  \"2017\":count_oa_2017} \n",
    "            \n",
    "            \n",
    "#             bucket_year_groups = OA_univ_papers_df.groupby(pd.cut(OA_univ_papers_df.year, pub_year_bins))\n",
    "#             bucket_year_groups_count_records = bucket_year_groups.size().to_dict()\n",
    "#             #  for easy readbility\n",
    "#             remapped = {}\n",
    "#             remapped[\"0-2010\"] = bucket_year_groups_count_records[pd.Interval(0, 2010, closed='right')]\n",
    "#             remapped[\"2011-2013\"] = bucket_year_groups_count_records[pd.Interval(2010, 2013, closed='right')]\n",
    "#             remapped[\"2014-2016\"] = bucket_year_groups_count_records[pd.Interval(2013, 2016, closed='right')]\n",
    "#             remapped[\"2017-2019\"] = bucket_year_groups_count_records[pd.Interval(2016, 2019, closed='right')]\n",
    "            \n",
    "#             univs_info[org_univ_name][\"yearwise_OA\"][\"count_intervals\"] = remapped\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             Note: This is the growth rate per year.\n",
    "#             growth_rate = {}  # change divided by the time it took to make that change\n",
    "#             growth_rate[\"2008\"] = (count_oa_2008 - count_oa_2007)/1.00\n",
    "#             growth_rate[\"2009\"] = (count_oa_2009 - count_oa_2008)/1.00\n",
    "#             growth_rate[\"2010\"] = (count_oa_2010 - count_oa_2009)/1.00\n",
    "#             growth_rate[\"2011\"] = (count_oa_2011 - count_oa_2010)/1.00\n",
    "#             growth_rate[\"2012\"] = (count_oa_2012 - count_oa_2011)/1.00\n",
    "#             growth_rate[\"2013\"] = (count_oa_2013 - count_oa_2012)/1.00\n",
    "#             growth_rate[\"2014\"] = (count_oa_2014 - count_oa_2013)/1.00\n",
    "#             growth_rate[\"2015\"] = (count_oa_2015 - count_oa_2014)/1.00\n",
    "#             growth_rate[\"2016\"] = (count_oa_2016 - count_oa_2015)/1.00\n",
    "#             growth_rate[\"2017\"] = (count_oa_2017 - count_oa_2016)/1.00\n",
    "#             growth_rate[\"2018\"] = (count_oa_2018 - count_oa_2017)/1.00\n",
    "            \n",
    "            \n",
    "#             univs_info[org_univ_name][\"yearwise_OA\"][\"growth_rate\"] = growth_rate\n",
    "    \n",
    "    bar_fig = create_OA_percent_bar_chart(univs_oa_percent, save_fname = join(output_dir,country_name+\"_\"+'OA_percent') , x_label = (\"Universities in \"+country_name), plt_text = ('Total Count of Universities = '+str(len(univs_oa_percent))) )\n",
    "    return bar_fig, univs_info, univs_not_found, univs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries_plot = {}\n",
    "all_countries_all_univs_OA_info = {}\n",
    "all_countries_univs_found_not_found = {}\n",
    "\n",
    "for country_name,univs_name in cfg['data']['all_THE_WUR_institutions_by_country'].items():\n",
    "    print(\"\\nProcesing for dataset of univs in \"+country_name+\"\\n\")\n",
    "    all_countries_plot[country_name] = {}\n",
    "    all_countries_univs_found_not_found[country_name] =  {}\n",
    "    \n",
    "    # CSV has repeated header from multiple partitions of the merge on pyspark csv output. Hence need to treat as string.\n",
    "    country_papers_OA_df = pd.read_csv(join(root,\"data/processed/OA_status_\"+country_name+\"_papers.csv\"), header=0, sep=\",\", dtype={'is_OA': object, \"url_lists_as_string\": object, \"year\": object, \"wikipage\": object})  # object means string\n",
    "    # Then eliminate problematic lines\n",
    "    #  temp fix until spark csv merge header issue is resolved -- the header line is present in each re-partition's output csv\n",
    "    country_papers_OA_df.drop(country_papers_OA_df[country_papers_OA_df.paperid == \"paperid\"].index, inplace=True)\n",
    "    # Then reset dtypes as needed.\n",
    "    country_papers_OA_df = country_papers_OA_df.astype({'year':int})  # todo : for other types too including is_OA and update the check method to boolean type\n",
    "    \n",
    "    # Finally, create a new column named normalizedwikiname. This is helpful for matching english names of non-english universities. Eg: get \"federal university of health sciences of porto alegre\" for \"universidade federal de ciencias da saude de porto alegre\" using the wikilink which contains \"universidade federal de ciencias da saude de porto alegre\" in it.\n",
    "    country_papers_OA_df[\"normalizedwikiname\"] = country_papers_OA_df['wikipage'].apply(mag_wiki_link_normalise)\n",
    "    \n",
    "    \n",
    "    country_plot, univs_info, univs_not_found, univs_found = get_plt_univ_papers_OA_stats(country_papers_OA_df, univs_name)\n",
    "    \n",
    "    all_countries_plot[country_name] =  country_plot\n",
    "    all_countries_all_univs_OA_info[country_name] =  univs_info\n",
    "    \n",
    "    count_total_univs = len(univs_not_found) + len(univs_found)\n",
    "    \n",
    "    not_found_details = {}\n",
    "    not_found_details['univ_names'] = univs_not_found\n",
    "    not_found_details['count_univs'] = len(univs_not_found)\n",
    "    not_found_details['percent_univs'] = (len(univs_not_found)*100.00)/count_total_univs\n",
    "    \n",
    "    found_details = {}\n",
    "    found_details['univ_names'] = univs_found\n",
    "    found_details['count_univs'] = len(univs_found)\n",
    "    found_details['percent_univs'] = (len(univs_found)*100.00)/count_total_univs\n",
    "    \n",
    "    \n",
    "    all_details = {}\n",
    "    all_details['count_univs'] = count_total_univs\n",
    "    \n",
    "    all_countries_univs_found_not_found[country_name]['not_found'] = not_found_details\n",
    "    all_countries_univs_found_not_found[country_name]['found'] = found_details\n",
    "    all_countries_univs_found_not_found[country_name]['all'] = all_details\n",
    "    \n",
    "    \n",
    "        \n",
    "    print(\"Saved plot for dataset of \"+country_name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text files with the infos\n",
    "\n",
    "with open(join(output_dir,'all_countries_univs_found_not_found.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_univs_found_not_found, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "        \n",
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_all_univs_OA_info, sort_keys=True, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from previously saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt')) as file:\n",
    "     all_countries_all_univs_OA_info = json.load(file)\n",
    "        \n",
    "# all_countries_all_univs_OA_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Representative universities OA percent comparision Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representative_univs_line_plot_groups(all_countries_all_univs_OA_info, save_fname, x_label=None, y_label = \"Percentage of OA Papers Published\", plt_text=None):\n",
    "\n",
    "    country_rep_univs = {}\n",
    "    \n",
    "    width = 0.9\n",
    "    \n",
    "    colors = (\"red\", \"blue\", \"green\")\n",
    "    groups = (\"Low Research Intensive Universities\", \"Medium Research Intensive Universities\", \"High Research Intensive Universities\")\n",
    "    \n",
    "    \n",
    "    high_tier_plot_data = []\n",
    "    mid_tier_plot_data = []\n",
    "    low_tier_plot_data = []\n",
    "    \n",
    "    \n",
    "    country_tier_mean_values = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for country, univ_tiers in cfg[\"data\"][\"research_intensive_THE_WUR_institutions_by_country\"].items():\n",
    "#         print(country)\n",
    "        country_rep_univs[cnames_for_plot[country]] = {}\n",
    "    \n",
    "        country_rep_univs[cnames_for_plot[country]][\"High_Tier\"]={}\n",
    "        country_rep_univs[cnames_for_plot[country]][\"Mid_Tier\"]={}\n",
    "        country_rep_univs[cnames_for_plot[country]][\"Low_Tier\"]={}\n",
    "        \n",
    "\n",
    "        high_tier_univs = univ_tiers[\"high\"]\n",
    "        for x in high_tier_univs:\n",
    "            high_tier_plot_data.append((country+\"(High)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"High_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "            \n",
    "    \n",
    "        medium_tier_univs = univ_tiers[\"medium\"]\n",
    "        for x in medium_tier_univs:\n",
    "            mid_tier_plot_data.append((country+\"(Mid)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"Mid_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "            \n",
    "        \n",
    "        low_tier_univs = univ_tiers[\"low\"]\n",
    "        for x in low_tier_univs:\n",
    "            low_tier_plot_data.append((country+\"(Low)\",all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]))\n",
    "            country_rep_univs[cnames_for_plot[country]][\"Low_Tier\"][x] = all_countries_all_univs_OA_info[country][x][\"percent_OA_papers\"]\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(15,10), sharex=True, sharey=True)\n",
    "    \n",
    "    sorted_cnames = sorted(cfg[\"data\"][\"research_intensive_THE_WUR_institutions_by_country\"].keys())\n",
    "    \n",
    "    hidden_tick_indices = []\n",
    "    count_hidden_tick_index = -1\n",
    "    for i in range(len(sorted_cnames)):\n",
    "        cname = sorted_cnames[i]\n",
    "        \n",
    "        # First plot the data for low tier univs of the country\n",
    "        country_low_tier_univs_values = [x[1] for x in low_tier_plot_data if x[0]==cname+\"(Low)\"]\n",
    "#         axs.plot([cname+\"(Low)\"]*len(country_low_tier_univs_values), country_low_tier_univs_values, c=\"red\", label=\"Low Tier University\", linestyle='-', marker='o', linewidth=4)\n",
    "\n",
    "        \n",
    "        country_low_tier_mean_value = mean(country_low_tier_univs_values)\n",
    "        country_low_tier_min_value = min(country_low_tier_univs_values)\n",
    "        country_low_tier_max_value = max(country_low_tier_univs_values)\n",
    "        country_tier_mean_values.append((cname+\"(Low)\",country_low_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"Low_Tier\"][\"Mean\"] = country_low_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cname+\"(Low)\"]*len(country_low_tier_univs_values), country_low_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_low_tier_max_value - country_low_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+1-width/2,country_low_tier_min_value-1) ,width=width, height=height+2, linewidth=1, color='cornflowerblue', fill=\"cornflowerblue\", alpha=0.25, label=\"Low Tier Universities\"))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Then plot the data for mid tier univs of the country    \n",
    "        country_mid_tier_univs_values = [x[1] for x in mid_tier_plot_data if x[0]==cname+\"(Mid)\"]\n",
    "#         axs.plot([cnames_for_plot[cname]]*len(country_mid_tier_univs_values), country_mid_tier_univs_values, c=\"orange\", label=\"Mid Tier University\", linestyle='-', marker='o', linewidth=4)  # to make this tick mark visible as cname rather than the true cname_mid; also capitalize the first letter\n",
    "\n",
    "        \n",
    "    \n",
    "        country_mid_tier_mean_value = mean(country_mid_tier_univs_values)\n",
    "        country_mid_tier_min_value = min(country_mid_tier_univs_values)\n",
    "        country_mid_tier_max_value = max(country_mid_tier_univs_values)\n",
    "        country_tier_mean_values.append((cnames_for_plot[cname],country_mid_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"Mid_Tier\"][\"Mean\"] = country_mid_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cnames_for_plot[cname]]*len(country_mid_tier_univs_values), country_mid_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_mid_tier_max_value - country_mid_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+2-width/2,country_mid_tier_min_value-1) ,width=width, height=height+2, linewidth=1, color='orange', fill=\"orange\", alpha=0.25, label=\"Mid Tier Universities\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # Also, plot the data for high tier univs of the country\n",
    "        country_high_tier_univs_values = [x[1] for x in high_tier_plot_data if x[0]==cname+\"(High)\"]\n",
    "#         axs.plot([cname+\"(High)\"]*len(country_high_tier_univs_values), country_high_tier_univs_values, c=\"green\", label=\"High Tier University\", linestyle='-', marker='o', linewidth=4)\n",
    "        \n",
    "        \n",
    "        country_high_tier_mean_value = mean(country_high_tier_univs_values)\n",
    "        country_high_tier_min_value = min(country_high_tier_univs_values)\n",
    "        country_high_tier_max_value = max(country_high_tier_univs_values)\n",
    "        country_tier_mean_values.append((cname+\"(High)\",country_high_tier_mean_value))\n",
    "        country_rep_univs[cnames_for_plot[cname]][\"High_Tier\"][\"Mean\"] = country_high_tier_mean_value\n",
    "        \n",
    "        \n",
    "        axs.scatter([cname+\"(High)\"]*len(country_high_tier_univs_values), country_high_tier_univs_values, c=\"black\", marker='x', label=\"OA %\")\n",
    "        height = country_high_tier_max_value - country_high_tier_min_value\n",
    "        axs.add_patch(Rectangle(xy=(count_hidden_tick_index+3-width/2,country_high_tier_min_value-1),width=width, height=height+2, linewidth=1, color='green', fill=\"green\", alpha=0.25, label=\"High Tier Universities\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Hide the tick marks for the low and high tier markers\n",
    "        hidden_tick_indices.append(count_hidden_tick_index+1)  # low marker\n",
    "        hidden_tick_indices.append(count_hidden_tick_index+3)  # high marker\n",
    "        \n",
    "        \n",
    "        # Finally add three fake tick points for inter spacing among the groups\n",
    "        if i!=(len(sorted_cnames)-1):  # except when the last true xticks have been added.\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 4\n",
    "            axs.plot([cname+\"(None1)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 1\n",
    "            axs.plot([cname+\"(None2)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "\n",
    "            count_hidden_tick_index = count_hidden_tick_index + 1\n",
    "            axs.plot([cname+\"(None3)\"], 10.0, c=\"white\", linestyle='-', marker='o')\n",
    "            hidden_tick_indices.append(count_hidden_tick_index)\n",
    "        \n",
    "        \n",
    "    \n",
    "#     https://stackoverflow.com/a/13583251/530399\n",
    "    xticks = axs.xaxis.get_major_ticks()\n",
    "    for hidden_tick_index in hidden_tick_indices:\n",
    "        xticks[hidden_tick_index].set_visible(False)\n",
    "    \n",
    "    \n",
    "    # Plot the mean value line\n",
    "#     axs.scatter(*zip(*country_tier_mean_values), label='Mean Value', s=280, facecolors='none', edgecolors='b')\n",
    "    axs.scatter(*zip(*country_tier_mean_values), label='Mean OA %', c=\"red\", marker='s', s=104)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # show grid at every ticks\n",
    "#     plt.grid()\n",
    "# https://stackoverflow.com/a/39039520/530399\n",
    "    axs.set_axisbelow(True)\n",
    "    axs.yaxis.grid(color='lightgrey', linestyle='dashed')\n",
    "    \n",
    "\n",
    "    # Frequency of y-ticks\n",
    "    # https://stackoverflow.com/a/12608937/530399\n",
    "    stepsize=3\n",
    "    start, end = axs.get_ylim()\n",
    "    axs.yaxis.set_ticks(np.arange(1, end, stepsize))\n",
    "    \n",
    "    # Font size to use for ticks\n",
    "    axs.xaxis.set_tick_params(labelsize=20)\n",
    "    axs.yaxis.set_tick_params(labelsize=20)\n",
    "    \n",
    "\n",
    "    axs.set_ylabel(y_label, fontsize=24, labelpad=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #     Remove multiple legends by unique entires. Because each country was separately adeed for each tiers, there are duplicate legend entries.\n",
    "#     https://stackoverflow.com/a/13589144/530399\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), prop={'size': 16},\n",
    "               loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True\n",
    "              )  # location of legend -- https://stackoverflow.com/a/4701285/530399\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return fig, country_rep_univs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_univ_OA_plot, country_rep_univs_data = create_representative_univs_line_plot_groups(all_countries_all_univs_OA_info, save_fname = join(output_dir,\"all_countries_representative_univs_OA_percent\"))\n",
    "\n",
    "rep_univ_OA_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write country_rep_univs to file\n",
    "with open(join(output_dir,'representative_univs_in_all_countries.txt'), 'w') as file:\n",
    "     file.write(json.dumps(country_rep_univs_data, sort_keys=True, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Analysis at Country Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can't build up on the data from univ_level because of duplicate paper. If the same paper(paperid) has authors from multiple univs within the same country, only one instance of it can be considered. \n",
    "\n",
    "#### 1. Load country level dataset 2. Retain records from unis in THE_WUR list only. 3. Delete duplicate paperid records 4. records from study_years only 4. Yearwise Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_info = {}\n",
    "countries_oa_percents = {}  # needed for plot.\n",
    "\n",
    "for country_name,univs_name in cfg['data']['all_THE_WUR_institutions_by_country'].items():\n",
    "    \n",
    "    countries_oa_info[country_name] = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. Load Data\n",
    "    # CSV has repeated header from multiple partitions of the merge on pyspark csv output. Hence need to treat as string.\n",
    "    country_papers_OA_df = pd.read_csv(join(root,\"data/processed/OA_status_\"+country_name+\"_papers.csv\"), header=0, sep=\",\", dtype={'is_OA': object, \"url_lists_as_string\": object, \"year\": object, \"wikipage\": object})  # object means string\n",
    "    # Then eliminate problematic lines\n",
    "    #  temp fix until spark csv merge header issue is resolved -- the header line is present in each re-partition's output csv\n",
    "    country_papers_OA_df.drop(country_papers_OA_df[country_papers_OA_df.paperid == \"paperid\"].index, inplace=True)\n",
    "    # Then reset dtypes as needed.\n",
    "    country_papers_OA_df = country_papers_OA_df.astype({'year':int})  # todo : for other types too including is_OA and update the check method to boolean type\n",
    "    \n",
    "    \n",
    "    # Finally, create a new column named normalizedwikiname. This is helpful for matching english names of non-english universities. Eg: get \"federal university of health sciences of porto alegre\" for \"universidade federal de ciencias da saude de porto alegre\" using the wikilink which contains \"universidade federal de ciencias da saude de porto alegre\" in it.\n",
    "    country_papers_OA_df[\"normalizedwikiname\"] = country_papers_OA_df['wikipage'].apply(mag_wiki_link_normalise)\n",
    "    \n",
    "    \n",
    "    # 2. Retain records from THE_WUR only\n",
    "    univs_names_normalized = [mag_normalisation_institution_names(x) for x in univs_name]\n",
    "    country_THE_papers_OA_df_set1 = country_papers_OA_df[country_papers_OA_df['normalizedname'].isin(univs_names_normalized)]\n",
    "    country_THE_papers_OA_df_set2 = country_papers_OA_df[country_papers_OA_df['normalizedwikiname'].isin(univs_names_normalized)]\n",
    "    # The records in two sets can be the excatly the same \n",
    "# Concat and remove exact duplicates  -- https://stackoverflow.com/a/21317570/530399\n",
    "    country_THE_papers_OA_df = pd.concat([country_THE_papers_OA_df_set1, country_THE_papers_OA_df_set2]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # 3. Remove Duplicates paperids -- same paper with authors from multiple universities within the country.\n",
    "    country_THE_papers_OA_df = country_THE_papers_OA_df.drop_duplicates(subset=\"paperid\")\n",
    "    \n",
    "    # 4. Put criteria that these papers are from 2007 till 2017\n",
    "    country_THE_papers_OA_df = country_THE_papers_OA_df[country_THE_papers_OA_df['year'].isin(study_years)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    OA_papers = country_THE_papers_OA_df[country_THE_papers_OA_df['is_OA']==\"true\"]\n",
    "    unknown_papers = country_THE_papers_OA_df[country_THE_papers_OA_df['is_OA']!=\"true\"]\n",
    "    \n",
    "    \n",
    "    count_country_OA_papers = len(OA_papers)\n",
    "    count_country_unknown_papers = len(unknown_papers)\n",
    "    \n",
    "    total_country_papers = count_country_OA_papers + count_country_unknown_papers\n",
    "    percent_OA_country = (count_country_OA_papers * 100.00)/total_country_papers\n",
    "    percent_unknown_country = (count_country_unknown_papers * 100.00)/total_country_papers\n",
    "    \n",
    "    \n",
    "    countries_oa_percents[country_name] = percent_OA_country\n",
    "    \n",
    "    countries_oa_info[country_name]['count_OA_papers'] = count_country_OA_papers\n",
    "    countries_oa_info[country_name]['count_unknown_papers'] = count_country_unknown_papers    \n",
    "    countries_oa_info[country_name]['percent_OA_papers'] = percent_OA_country\n",
    "    countries_oa_info[country_name]['percent_unknown_papers'] = percent_unknown_country\n",
    "    countries_oa_info[country_name]['count_total_papers'] = total_country_papers\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Yearwise Breakdown\n",
    "    count_oa_2007 = len(OA_papers[OA_papers[\"year\"]==2007])\n",
    "    count_oa_2008 = len(OA_papers[OA_papers[\"year\"]==2008])\n",
    "    count_oa_2009 = len(OA_papers[OA_papers[\"year\"]==2009])\n",
    "    count_oa_2010 = len(OA_papers[OA_papers[\"year\"]==2010])\n",
    "    count_oa_2011 = len(OA_papers[OA_papers[\"year\"]==2011])\n",
    "    count_oa_2012 = len(OA_papers[OA_papers[\"year\"]==2012])\n",
    "    count_oa_2013 = len(OA_papers[OA_papers[\"year\"]==2013])\n",
    "    count_oa_2014 = len(OA_papers[OA_papers[\"year\"]==2014])\n",
    "    count_oa_2015 = len(OA_papers[OA_papers[\"year\"]==2015])\n",
    "    count_oa_2016 = len(OA_papers[OA_papers[\"year\"]==2016])\n",
    "    count_oa_2017 = len(OA_papers[OA_papers[\"year\"]==2017])\n",
    "   \n",
    "\n",
    "    \n",
    "    count_all_2007 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2007])\n",
    "    count_all_2008 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2008])\n",
    "    count_all_2009 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2009])\n",
    "    count_all_2010 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2010])\n",
    "    count_all_2011 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2011])\n",
    "    count_all_2012 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2012])\n",
    "    count_all_2013 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2013])\n",
    "    count_all_2014 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2014])\n",
    "    count_all_2015 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2015])\n",
    "    count_all_2016 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2016])\n",
    "    count_all_2017 = len(country_THE_papers_OA_df[country_THE_papers_OA_df[\"year\"]==2017])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    countries_oa_info[country_name][\"yearwise_OA\"] = {}\n",
    "    \n",
    "    countries_oa_info[country_name][\"yearwise_OA\"][\"count_year\"] = {\"2007\":count_oa_2007, \"2008\":count_oa_2008,\n",
    "                                                           \"2009\":count_oa_2009, \"2010\":count_oa_2010,\n",
    "                                                           \"2011\":count_oa_2011, \"2012\":count_oa_2012,\n",
    "                                                           \"2013\":count_oa_2013, \"2014\":count_oa_2014,\n",
    "                                                           \"2015\":count_oa_2015, \"2016\":count_oa_2016,\n",
    "                                                           \"2017\":count_oa_2017}\n",
    "\n",
    "    \n",
    "    # Lets find the percentage OA in each year\n",
    "    countries_oa_info[country_name][\"yearwise_OA\"][\"percent_year\"] = {\n",
    "        \"2007\":(count_oa_2007*100.00)/count_all_2007,\n",
    "        \"2008\":(count_oa_2008*100.00)/count_all_2008,\n",
    "       \"2009\":(count_oa_2009*100.00)/count_all_2009,\n",
    "        \"2010\":(count_oa_2010*100.00)/count_all_2010,\n",
    "       \"2011\":(count_oa_2011*100.00)/count_all_2011,\n",
    "        \"2012\":(count_oa_2012*100.00)/count_all_2012,\n",
    "       \"2013\":(count_oa_2013*100.00)/count_all_2013,\n",
    "        \"2014\":(count_oa_2014*100.00)/count_all_2014,\n",
    "       \"2015\":(count_oa_2015*100.00)/count_all_2015, \n",
    "        \"2016\":(count_oa_2016*100.00)/count_all_2016,\n",
    "       \"2017\":(count_oa_2017*100.00)/count_all_2017\n",
    "    }\n",
    "    \n",
    "    \n",
    "    print(\"\\nCompleted procesing for dataset of \"+country_name+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''countries_oa_info = {}\n",
    "countries_oa_percents = {}  # needed for plot.\n",
    "\n",
    "for key,val in all_countries_all_univs_OA_info.items():\n",
    "#     print(key)\n",
    "    count_country_OA_papers = 0\n",
    "    count_country_unknown_papers = 0\n",
    "    \n",
    "#     count_yearwise_0_to_2010 = 0\n",
    "#     count_yearwise_2011_to_2013 = 0\n",
    "#     count_yearwise_2014_to_2016 = 0\n",
    "#     count_yearwise_2017_to_2019 = 0\n",
    "    \n",
    "    \n",
    "    count_oa_2007 = 0\n",
    "    count_oa_2008 = 0\n",
    "    count_oa_2009 = 0\n",
    "    count_oa_2010 = 0\n",
    "    count_oa_2011 = 0\n",
    "    count_oa_2012 = 0\n",
    "    count_oa_2013 = 0\n",
    "    count_oa_2014 = 0\n",
    "    count_oa_2015 = 0\n",
    "    count_oa_2016 = 0\n",
    "    count_oa_2017 = 0\n",
    "    count_oa_2018 = 0\n",
    "    \n",
    "    \n",
    "    count_all_2008 = 0\n",
    "    count_all_2009 = 0\n",
    "    count_all_2010 = 0\n",
    "    count_all_2011 = 0\n",
    "    count_all_2012 = 0\n",
    "    count_all_2013 = 0\n",
    "    count_all_2014 = 0\n",
    "    count_all_2015 = 0\n",
    "    count_all_2016 = 0\n",
    "    count_all_2017 = 0\n",
    "    count_all_2018 = 0\n",
    "    \n",
    "    \n",
    "    for univ_name,univ_oa_details in val.items():\n",
    "        count_country_OA_papers = count_country_OA_papers + univ_oa_details['count_OA_papers']\n",
    "        count_country_unknown_papers = count_country_unknown_papers + univ_oa_details['count_unknown_papers']\n",
    "        \n",
    "        # Lets get the sum of count of OA in the selected intervals\n",
    "#         count_yearwise_0_to_2010 = count_yearwise_0_to_2010 + univ_oa_details[\"yearwise_OA\"][\"count_intervals\"][\"0-2010\"]\n",
    "#         count_yearwise_2011_to_2013 = count_yearwise_2011_to_2013 + univ_oa_details[\"yearwise_OA\"][\"count_intervals\"][\"2011-2013\"]\n",
    "#         count_yearwise_2014_to_2016 = count_yearwise_2014_to_2016 + univ_oa_details[\"yearwise_OA\"][\"count_intervals\"][\"2014-2016\"]\n",
    "#         count_yearwise_2017_to_2019 = count_yearwise_2017_to_2019 + univ_oa_details[\"yearwise_OA\"][\"count_intervals\"][\"2017-2019\"]\n",
    "        \n",
    "        # The sum of count of oa in specific years -- needed to find growth rate\n",
    "        count_oa_2007 = count_oa_2007 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2007\"]\n",
    "        count_oa_2008 = count_oa_2008 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2008\"]\n",
    "        count_oa_2009 = count_oa_2009 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2009\"]\n",
    "        count_oa_2010 = count_oa_2010 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2010\"]\n",
    "        count_oa_2011 = count_oa_2011 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2011\"]\n",
    "        count_oa_2012 = count_oa_2012 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2012\"]\n",
    "        count_oa_2013 = count_oa_2013 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2013\"]\n",
    "        count_oa_2014 = count_oa_2014 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2014\"]\n",
    "        count_oa_2015 = count_oa_2015 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2015\"]\n",
    "        count_oa_2016 = count_oa_2016 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2016\"]\n",
    "        count_oa_2017 = count_oa_2017 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2017\"]\n",
    "        count_oa_2018 = count_oa_2018 + univ_oa_details[\"yearwise_OA\"][\"count_year\"][\"2018\"]\n",
    "        \n",
    "        \n",
    "        # The sum of count of all papers in specific years -- needed to find oa percent within each year\n",
    "        count_all_2008 = count_all_2008 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2008\"]\n",
    "        count_all_2009 = count_all_2009 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2009\"]\n",
    "        count_all_2010 = count_all_2010 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2010\"]\n",
    "        count_all_2011 = count_all_2011 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2011\"]\n",
    "        count_all_2012 = count_all_2012 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2012\"]\n",
    "        count_all_2013 = count_all_2013 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2013\"]\n",
    "        count_all_2014 = count_all_2014 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2014\"]\n",
    "        count_all_2015 = count_all_2015 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2015\"]\n",
    "        count_all_2016 = count_all_2016 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2016\"]\n",
    "        count_all_2017 = count_all_2017 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2017\"]\n",
    "        count_all_2018 = count_all_2018 + univ_oa_details[\"yearwise_all\"][\"count_year\"][\"2018\"]\n",
    "    \n",
    "    \n",
    "    total_country_papers = count_country_OA_papers + count_country_unknown_papers\n",
    "    percent_OA_country = (count_country_OA_papers * 100.00)/total_country_papers\n",
    "    percent_unknown_country = (count_country_unknown_papers * 100.00)/total_country_papers\n",
    "    \n",
    "    countries_oa_info[key] = {}\n",
    "    countries_oa_info[key]['count_OA_papers'] = count_country_OA_papers\n",
    "    countries_oa_info[key]['count_unknown_papers'] = count_country_unknown_papers    \n",
    "    countries_oa_info[key]['percent_OA_papers'] = percent_OA_country\n",
    "    countries_oa_info[key]['percent_unknown_papers'] = percent_unknown_country\n",
    "    countries_oa_info[key]['count_total_papers'] = total_country_papers\n",
    "    \n",
    "    countries_oa_info[key][\"yearwise_OA\"] = {}\n",
    "    \n",
    "#     countries_oa_info[key][\"yearwise_OA\"][\"count_intervals\"] = {\"0-2010\": count_yearwise_0_to_2010,\n",
    "#                                                                 \"2011-2013\": count_yearwise_2011_to_2013,\n",
    "#                                                                \"2014-2016\": count_yearwise_2014_to_2016,\n",
    "#                                                                 \"2017-2019\": count_yearwise_2017_to_2019}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    countries_oa_info[key][\"yearwise_OA\"][\"count_year\"] = {\"2007\":count_oa_2007, \"2008\":count_oa_2008,\n",
    "                                                           \"2009\":count_oa_2009, \"2010\":count_oa_2010,\n",
    "                                                           \"2011\":count_oa_2011, \"2012\":count_oa_2012,\n",
    "                                                           \"2013\":count_oa_2013, \"2014\":count_oa_2014,\n",
    "                                                           \"2015\":count_oa_2015, \"2016\":count_oa_2016,\n",
    "                                                           \"2017\":count_oa_2017, \"2018\":count_oa_2018} \n",
    "    \n",
    "    # Lets find the percentage OA in each year\n",
    "    countries_oa_info[key][\"yearwise_OA\"][\"percent_year\"] = {\n",
    "        \"2008\":(count_oa_2008*100.00)/count_all_2008,\n",
    "       \"2009\":(count_oa_2009*100.00)/count_all_2009,\n",
    "        \"2010\":(count_oa_2010*100.00)/count_all_2010,\n",
    "       \"2011\":(count_oa_2011*100.00)/count_all_2011,\n",
    "        \"2012\":(count_oa_2012*100.00)/count_all_2012,\n",
    "       \"2013\":(count_oa_2013*100.00)/count_all_2013,\n",
    "        \"2014\":(count_oa_2014*100.00)/count_all_2014,\n",
    "       \"2015\":(count_oa_2015*100.00)/count_all_2015, \n",
    "        \"2016\":(count_oa_2016*100.00)/count_all_2016,\n",
    "       \"2017\":(count_oa_2017*100.00)/count_all_2017,\n",
    "        \"2018\":(count_oa_2018*100.00)/count_all_2018\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Lets also find the growth within each intervals\n",
    "#     growth_rate = {}  # change divided by the time it took to make that change\n",
    "#     growth_rate[\"2008\"] = (count_oa_2008 - count_oa_2007)/1.00\n",
    "#     growth_rate[\"2009\"] = (count_oa_2009 - count_oa_2008)/1.00\n",
    "#     growth_rate[\"2010\"] = (count_oa_2010 - count_oa_2009)/1.00\n",
    "#     growth_rate[\"2011\"] = (count_oa_2011 - count_oa_2010)/1.00\n",
    "#     growth_rate[\"2012\"] = (count_oa_2012 - count_oa_2011)/1.00\n",
    "#     growth_rate[\"2013\"] = (count_oa_2013 - count_oa_2012)/1.00\n",
    "#     growth_rate[\"2014\"] = (count_oa_2014 - count_oa_2013)/1.00\n",
    "#     growth_rate[\"2015\"] = (count_oa_2015 - count_oa_2014)/1.00\n",
    "#     growth_rate[\"2016\"] = (count_oa_2016 - count_oa_2015)/1.00\n",
    "#     growth_rate[\"2017\"] = (count_oa_2017 - count_oa_2016)/1.00\n",
    "#     growth_rate[\"2018\"] = (count_oa_2018 - count_oa_2017)/1.00\n",
    "#     countries_oa_info[key][\"yearwise_OA\"][\"growth_rate\"] = growth_rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    countries_oa_percents[key] = percent_OA_country'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(output_dir,'all_countries_OA_info.txt'), 'w') as file:\n",
    "     file.write(json.dumps(countries_oa_info, sort_keys=True, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percent_bar_plot = create_OA_percent_bar_chart(countries_oa_percents, save_fname = join(output_dir,\"all_countries_OA_percent\"), x_label = \"Countries\", display_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_percent_bar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This data will be put in latex table -- plot not needed\n",
    "def create_triple_bar_chart(oa_info_dict, save_fname, x_label=None):\n",
    "    '''\n",
    "    Contains plot of count OA, count Unknown and count Total. The % OA value is too small for this plot and is therefore separately shown in the other graph.\n",
    "    '''\n",
    "    \n",
    "    #  Sort by Percentage OA   \n",
    "    #  https://stackoverflow.com/a/37266356/530399\n",
    "    sort_by_vals = sorted(oa_info_dict.items(), key=lambda kv: kv[0]) # sorted by keys, return a list of tuples\n",
    "    \n",
    "    names = [x for (x,y) in sort_by_vals]\n",
    "    \n",
    "    # set width of bar\n",
    "    barWidth = 0.3\n",
    "    \n",
    "    \n",
    "    y_values_scale_down_factor = 10000\n",
    "    # set height of bar -- scale down by 10k   \n",
    "    bars_oa_count = [y['count_OA_papers']/y_values_scale_down_factor for (x,y) in sort_by_vals]\n",
    "    bars_unknown_count = [y['count_unknown_papers']/y_values_scale_down_factor for (x,y) in sort_by_vals]\n",
    "    bars_total_count = [(y['count_OA_papers']+y['count_unknown_papers'])/y_values_scale_down_factor for (x,y) in sort_by_vals]\n",
    "    \n",
    "    \n",
    "    sep_between_group_bars = 2\n",
    "    # Set position of bar on X axis\n",
    "    r1 = np.arange(len(bars_oa_count))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    plt.bar(r1, bars_oa_count, color='blue', width=barWidth, edgecolor='white', label='Count OA')\n",
    "    plt.bar(r2, bars_unknown_count, color='red', width=barWidth, edgecolor='white', label='Count Unknown')\n",
    "    plt.bar(r3, bars_total_count, color='green', width=barWidth, edgecolor='white', label='Total Papers Count')\n",
    "\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Count of OA, Unknown and Total Papers (Expressed in terms of 10K) \")\n",
    "    \n",
    "    #     https://stackoverflow.com/a/8482667/530399\n",
    "#     plt.text(0.7, 0.9,\"Numbers are in terms of 10K\", ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        x = p.get_x()+p.get_width()/2.\n",
    "        ax.annotate(\"%g\" % h, xy=(x,h), xytext=(0,4), rotation=90,  textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n",
    "    \n",
    "    \n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xticks([r + barWidth for r in range(len(bars_oa_count))], names, rotation='vertical')\n",
    "    \n",
    "    \n",
    "    # Create legend & Show graphic\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "countries_oa_info_bar_plot = create_triple_bar_chart(countries_oa_info, save_fname = join(output_dir,\"all_countries_OA_info\"), x_label = \"Countries\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "countries_oa_info_bar_plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''def create_OA_growth_line_chart(countries_oa_info, save_fname, x_label = \"Year\", plt_text=None):\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    country_names_list = []\n",
    "    markers = ['o', 'x', 'v', 's', '*', '+', 'D', '|']\n",
    "    for country_name,oa_info in countries_oa_info.items():\n",
    "        \n",
    "        growth_rate = oa_info[\"yearwise_OA\"][\"growth_rate\"]\n",
    "        # sort by year\n",
    "        #  https://stackoverflow.com/a/37266356/530399\n",
    "        sort_by_year = sorted(growth_rate.items(), key=lambda kv: int(kv[0]))\n",
    "        years, growth_rates = zip(*sort_by_year) # unpack a list of pairs into two tuples\n",
    "        \n",
    "        plt.plot(years,growth_rates, linewidth=4, markersize=12, marker=markers[len(country_names_list)])\n",
    "        \n",
    "        country_names_list.append(country_name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Growth Rate of OA papers per year\")\n",
    "    \n",
    "    if plt_text:\n",
    "#     https://stackoverflow.com/a/8482667/530399\n",
    "        plt.text(0.7, 0.9,plt_text, ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "#     plt.xticks(years)\n",
    "    plt.legend(country_names_list, loc='upper left')\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''countries_OA_growth_line_plot = create_OA_growth_line_chart(countries_oa_info, save_fname = join(output_dir,\"all_countries_OA_growth\"), x_label = \"Year\")'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# countries_OA_growth_line_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yearwise_OA_percent_line_chart(countries_oa_info, save_fname, x_label = \"Year\", plt_text=None):\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    country_names_list = []\n",
    "    markers = ['o', 'x', 'v', 's', '*', '+', 'D', '|']\n",
    "    for country_name,oa_info in countries_oa_info.items():\n",
    "        \n",
    "        percent_oa = oa_info[\"yearwise_OA\"][\"percent_year\"]\n",
    "        # sort by year\n",
    "        #  https://stackoverflow.com/a/37266356/530399\n",
    "        sort_by_year = sorted(percent_oa.items(), key=lambda kv: int(kv[0]))\n",
    "        years, percent_oas = zip(*sort_by_year) # unpack a list of pairs into two tuples\n",
    "        \n",
    "        plt.plot(years, percent_oas, linewidth=4, markersize=12, marker=markers[len(country_names_list)])\n",
    "        \n",
    "        country_names_list.append(country_name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ax = plt.gca()\n",
    "    if x_label:\n",
    "        ax.set_xlabel(x_label, fontsize=20, labelpad=10)\n",
    "    ax.set_ylabel(\"% of OA paper published in each year\", fontsize=24, labelpad=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Font size to use for ticks\n",
    "    ax.xaxis.set_tick_params(labelsize=20)\n",
    "    ax.yaxis.set_tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    # Frequency of x-ticks\n",
    "    # https://stackoverflow.com/a/12608937/530399\n",
    "    stepsize=3\n",
    "    start, end = ax.get_ylim()\n",
    "    ax.yaxis.set_ticks(np.arange(int(start), end, stepsize))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # show grid at every ticks\n",
    "    #     plt.grid()\n",
    "    # https://stackoverflow.com/a/39039520/530399\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='lightgrey', linestyle='dashed')\n",
    "    \n",
    "    \n",
    "    if plt_text:\n",
    "#     https://stackoverflow.com/a/8482667/530399\n",
    "        plt.text(0.7, 0.9,plt_text, ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "#     plt.xticks(years)\n",
    "    plt.legend([cnames_for_plot[x] for x in country_names_list], loc='upper left', prop={'size': 16})\n",
    "    \n",
    "    plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "    plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_OA_percent_each_year_line_plot = create_yearwise_OA_percent_line_chart(countries_oa_info, save_fname = join(output_dir,\"all_countries_OA_percent_each_year\"), x_label = \"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_OA_percent_each_year_line_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_oa_info['usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_oa_info['brazil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_oa_info['germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\nCompleted!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
