{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will create plots for institutions of universities in THE WUR univs only and for the period of 2007-2017. The input dataset contains info of THE WUR univs only but for any period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unpaywall dump used was from (April or June) 2018; hence analysis until 2017 only is going to be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : What is the proportion of OA and non-OA papers published by each of the THE WUR universities within each country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard path wrangling to be able to import project config and sources\n",
    "import os\n",
    "import sys\n",
    "from os.path import join\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print('Project root: {}'.format(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(join(root,\"spark/shared/\"))\n",
    "from MAG_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "\n",
    "# Installed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc,rcParams\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = None\n",
    "with open(join(root,\"spark/config.json\")) as fp:\n",
    "    cfg = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = join(root,\"documents/analysis/dataset_selection_question1\")\n",
    "# Create a new directory to save results\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames_for_plot = {\n",
    "    \"austria\" : \"Austria\",\n",
    "    \"brazil\" : \"Brazil\",\n",
    "    \"germany\" : \"Germany\",\n",
    "    \"india\" : \"India\",\n",
    "    \"portugal\" : \"Portugal\",\n",
    "    \"russia\" : \"Russia\",\n",
    "    \"uk\" : \"UK\",\n",
    "    \"usa\" : \"USA\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_years = [2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Citation Counts of OA and unknown papers for each university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_univ_papers_citation_counts(country_papers_OA_df, univs_name):\n",
    "    '''\n",
    "    Get the count of OA and non-OA papers for each university in the input country\n",
    "    '''\n",
    "    univs_info = {}\n",
    "    \n",
    "    univs_not_found = []\n",
    "    univs_found = []\n",
    "    \n",
    "    for org_univ_name in set(univs_name):  # remove duplicate univ names in the THE list, if any\n",
    "#         print(org_univ_name)\n",
    "\n",
    "        THE_univ_name_normalised = mag_normalisation_institution_names(org_univ_name)\n",
    "    \n",
    "        '''\n",
    "        The dataframe that will be selected for the current univ is either :\n",
    "        1. When the MAG normalizedname column matches to THE_univ_name_normalised\n",
    "        or\n",
    "        2. When the MAG normalised(wikiname) matches to THE_univ_name_normalised -- this matches English names (in MAG wiki links as well as THE) of non English name (in MAG normalisedname or displayname) universities.\n",
    "        '''\n",
    "        univ_papers_df_set1 = country_papers_OA_df[country_papers_OA_df['normalizedname']==THE_univ_name_normalised]\n",
    "        \n",
    "        univ_papers_df_set2 = country_papers_OA_df[country_papers_OA_df['normalizedwikiname']==THE_univ_name_normalised]\n",
    "        \n",
    "        # The records in two sets can be the excatly the same \n",
    "        # Concat and remove exact duplicates  -- https://stackoverflow.com/a/21317570/530399\n",
    "        univ_papers_df = pd.concat([univ_papers_df_set1, univ_papers_df_set2]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "\n",
    "#         Put additional criteria that these papers are from 2007 till 2017\n",
    "        univ_papers_df = univ_papers_df[univ_papers_df['year'].isin(study_years)]\n",
    "        \n",
    "        \n",
    "        # Same paper will have multiple entries if there are multiple authors for that paper from same university.\n",
    "        # This is not necessary because the input dataset was already prepared to exclude such duplicates.\n",
    "#         univ_papers_df = univ_papers_df.drop_duplicates(subset=\"paperid\")\n",
    "\n",
    "\n",
    "        \n",
    "        count_total_univ_papers = len(univ_papers_df)\n",
    "        \n",
    "        \n",
    "        # For those I couldn't match/find their name, it is not fair to say that their OA count is 0. Should be excluded from the graph.\n",
    "        if count_total_univ_papers==0:\n",
    "            univs_not_found.append(org_univ_name+\"    @    \"+THE_univ_name_normalised)\n",
    "        else:\n",
    "            univs_found.append(org_univ_name)\n",
    "            \n",
    "            univs_info[org_univ_name] = {}\n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "            \n",
    "            \n",
    "            OA_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']==\"true\"] # stored as a string in csv\n",
    "            unknown_univ_papers_df = univ_papers_df[univ_papers_df['is_OA']!=\"true\"] # stored as a string in csv\n",
    "            \n",
    "            count_OA_univ_papers = len(OA_univ_papers_df)\n",
    "            count_unknown_univ_papers = len(unknown_univ_papers_df)\n",
    "\n",
    "            univ_oa_percent = (count_OA_univ_papers*100.00)/count_total_univ_papers\n",
    "            univ_other_percent = (count_unknown_univ_papers*100.00)/count_total_univ_papers\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            univs_info[org_univ_name][\"count_OA_papers\"] = count_OA_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_OA_papers\"] = univ_oa_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_unknown_papers\"] = count_unknown_univ_papers\n",
    "            univs_info[org_univ_name][\"percent_unknown_papers\"] = univ_other_percent\n",
    "            \n",
    "            univs_info[org_univ_name][\"count_total_papers\"] = count_total_univ_papers\n",
    "            \n",
    "        \n",
    "    return univs_info, univs_not_found, univs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries_all_univs_OA_info = {}\n",
    "all_countries_univs_found_not_found = {}\n",
    "\n",
    "for country_name,univs_name in cfg['data']['all_THE_WUR_institutions_by_country'].items():\n",
    "    print(\"\\nProcesing for dataset of univs in \"+country_name+\"\\n\")\n",
    "    all_countries_univs_found_not_found[country_name] =  {}\n",
    "    \n",
    "    input_csv_path = join(root,\"data/processed/oa_status_\"+country_name+\"_papers.csv\")\n",
    "    \n",
    "    # CSV has repeated header from multiple partitions of the merge on pyspark csv output. Hence need to treat as string.\n",
    "    country_papers_OA_df = pd.read_csv(input_csv_path, header=0, sep=\",\", dtype={'is_OA': object, \"url_lists_as_string\": object, \"year\": object, \"wikipage\": object, \"normalizedwikiname\": object})  # object means string\n",
    "    # Then eliminate problematic lines\n",
    "    #  temp fix until spark csv merge header issue is resolved -- the header line is present in each re-partition's output csv\n",
    "    country_papers_OA_df.drop(country_papers_OA_df[country_papers_OA_df.paperid == \"paperid\"].index, inplace=True)\n",
    "    # Then reset dtypes as needed.\n",
    "    country_papers_OA_df = country_papers_OA_df.astype({'year':int})  # todo : for other types too including is_OA and update the check method to boolean type\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    univs_info, univs_not_found, univs_found = get_univ_papers_citation_counts(country_papers_OA_df, univs_name)\n",
    "    \n",
    "    all_countries_all_univs_OA_info[country_name] =  univs_info\n",
    "    \n",
    "    count_total_univs = len(univs_not_found) + len(univs_found)\n",
    "    \n",
    "    not_found_details = {}\n",
    "    not_found_details['univ_names'] = univs_not_found\n",
    "    not_found_details['count_univs'] = len(univs_not_found)\n",
    "    not_found_details['percent_univs'] = (len(univs_not_found)*100.00)/count_total_univs\n",
    "    \n",
    "    found_details = {}\n",
    "    found_details['univ_names'] = univs_found\n",
    "    found_details['count_univs'] = len(univs_found)\n",
    "    found_details['percent_univs'] = (len(univs_found)*100.00)/count_total_univs\n",
    "    \n",
    "    \n",
    "    all_details = {}\n",
    "    all_details['count_univs'] = count_total_univs\n",
    "    \n",
    "    all_countries_univs_found_not_found[country_name]['not_found'] = not_found_details\n",
    "    all_countries_univs_found_not_found[country_name]['found'] = found_details\n",
    "    all_countries_univs_found_not_found[country_name]['all'] = all_details\n",
    "    \n",
    "    \n",
    "        \n",
    "    print(\"Computed OA counts for all univs in \"+country_name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text files with the infos\n",
    "\n",
    "with open(join(output_dir,'all_countries_univs_found_not_found.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_univs_found_not_found, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "        \n",
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt'), 'w') as file:\n",
    "     file.write(json.dumps(all_countries_all_univs_OA_info, sort_keys=True, indent=4, ensure_ascii=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from previously saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(output_dir,'all_countries_all_univs_OA_info.txt')) as file:\n",
    "     all_countries_all_univs_OA_info = json.load(file)\n",
    "        \n",
    "# all_countries_all_univs_OA_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create bar plot for each of the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_bar_with_value(ax, rects, value_labels):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for i in range(len(rects)):\n",
    "        rect = rects[i]\n",
    "        label_value = value_labels[i]\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*rect.get_height(),\n",
    "                '%s' % label_value,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "def create_citation_count_distribution_bar_chart(univs_details, save_fname, x_label, save_file=True):\n",
    "    # https://chrisalbon.com/python/data_visualization/matplotlib_grouped_bar_plot/\n",
    "    # https://stackoverflow.com/a/42498711/530399\n",
    "    \n",
    "    univs_name = [x for x in univs_details.keys()]\n",
    "    univs_data = univs_details.values()\n",
    "    univs_oa_percent = [x['percent_OA_papers'] for x in univs_data]\n",
    "    univs_unknown_percent = [x['percent_unknown_papers'] for x in univs_data]\n",
    "    \n",
    "    \n",
    "    raw_data = {'univs_name': univs_name,\n",
    "        'univs_oa_percent': univs_oa_percent,\n",
    "        'univs_unknown_percent': univs_unknown_percent\n",
    "               }\n",
    "    df = pd.DataFrame(raw_data, columns = ['univs_name', 'univs_oa_percent', 'univs_unknown_percent'])\n",
    "    \n",
    "    # Compute proportion of univs_oa_citation_counts\n",
    "#     df['proportion_univs_oa_citation_counts'] = (df['univs_oa_citation_counts'] / (df['univs_oa_citation_counts'] + df['univs_unknown_citation_counts'])) *100\n",
    "    # sort the df based on univs_oa_percent \n",
    "    df = df.sort_values('univs_oa_percent', ascending=False)[['univs_name', 'univs_oa_percent','univs_unknown_percent']]\n",
    "    \n",
    "    # Setting the positions and width for the bars\n",
    "    pos = list(range(len(df['univs_name']))) \n",
    "    width = 0.25 \n",
    "\n",
    "    # Plotting the bars\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "\n",
    "    # Create a bar with oa_proportion data,\n",
    "    # in position pos,\n",
    "    oa_proportion_bars = ax.bar(pos, \n",
    "            #using df['univs_oa_citation_counts'] data,\n",
    "            df['univs_oa_percent'], \n",
    "            # of width\n",
    "            width, \n",
    "            # with alpha 0.5\n",
    "            alpha=0.5, \n",
    "            # with color\n",
    "            color='green', \n",
    "            )\n",
    "    # Create labels for oa bars\n",
    "    oa_proportion_bars_value_labels = [str(int(x))+\"%\" for x in df['univs_oa_percent'].values.tolist()]\n",
    "\n",
    "    # Create a bar with unknown_citation_count data,\n",
    "    # in position pos + some width buffer,\n",
    "    plt.bar([p + width for p in pos], \n",
    "            #using df['univs_unknown_citation_counts'] data,\n",
    "            df['univs_unknown_percent'],\n",
    "            # of width\n",
    "            width, \n",
    "            # with alpha 0.5\n",
    "            alpha=0.5, \n",
    "            # with color\n",
    "            color='red', \n",
    "            ) \n",
    "\n",
    "    # Set the y axis label\n",
    "    ax.set_ylabel('Proportion of Papers Published')\n",
    "\n",
    "    # Set the x axis label\n",
    "    ax.set_xlabel(x_label)\n",
    "\n",
    "    # Set the position of the x ticks\n",
    "    ax.set_xticks([p + 0.5 * width for p in pos])\n",
    "\n",
    "    # Set the labels for the x ticks\n",
    "    ax.set_xticklabels(df['univs_name'], rotation='vertical')\n",
    "\n",
    "    # Setting the x-axis and y-axis limits\n",
    "    plt.xlim(min(pos)-width, max(pos)+width*4)\n",
    "    plt.ylim([0, max(df['univs_oa_percent'] + df['univs_unknown_percent'])] )\n",
    "\n",
    "    # Adding the legend and showing the plot\n",
    "    plt.legend(['OA papers Proportion', 'Unknown papers Proportion'], loc='upper left')\n",
    "    plt.grid()\n",
    "    \n",
    "    label_bar_with_value(ax, oa_proportion_bars, oa_proportion_bars_value_labels)\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_fname+\".png\", bbox_inches='tight', dpi=900)\n",
    "        plt.savefig(save_fname+\".pdf\", bbox_inches='tight', dpi=900)\n",
    "    \n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = 'brazil'\n",
    "univs_details = all_countries_all_univs_OA_info[country_name]\n",
    "\n",
    "create_citation_count_distribution_bar_chart(univs_details, save_fname = join(output_dir,country_name+\"_\"+'citationcount_distribution'), x_label = (\"Universities in \"+cnames_for_plot[country_name]), save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_name, univs_details in all_countries_all_univs_OA_info.items():\n",
    "    create_citation_count_distribution_bar_chart(univs_details, save_fname = join(output_dir,country_name+\"_\"+'oa_proportion'), x_label = (\"Universities in \"+cnames_for_plot[country_name]), save_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\nCompleted!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
